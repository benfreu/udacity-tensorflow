{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 1 - Deduplicate dataset that we saved to the pickle file\n",
    "------------\n",
    "\n",
    "## WARNING: Uses Python 3\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "**Here we check the dataset for duplicates and save another pickle without duplicate examples.**\n",
    "\n",
    "We use a simple hashing function that might give us false positives but no false negatives. (I.e. we potentially get rid of too many examples but the resulting set won't have duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000,)\n",
      "Validation set (10000, 784) (10000,)\n",
      "Test set (18724, 784) (18724,)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset):\n",
    "  return dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "\n",
    "train_dataset = reformat(train_dataset)\n",
    "valid_dataset = reformat(valid_dataset)\n",
    "test_dataset = reformat(test_dataset)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate simple hashes for all examples by multiplying each pixel-value with a random float and summing up the products\n",
    "r = np.random.rand(train_dataset.shape[1])\n",
    "train_hashes = np.tile(train_dataset.dot(r), (1, 1))\n",
    "valid_hashes = np.tile(valid_dataset.dot(r), (1, 1))\n",
    "test_hashes = np.tile(test_dataset.dot(r), (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deduplicate(dataset):\n",
    "    \"\"\"Find duplicates within the dataset and return only unique indices.\"\"\"\n",
    "    hashes = []\n",
    "    unique_indices = []\n",
    "    for i in range(dataset.shape[1]):\n",
    "        if dataset[0][i] not in hashes:\n",
    "            hashes.append(dataset[0][i])\n",
    "            unique_indices.append(i)\n",
    "    return np.array(unique_indices)\n",
    "        \n",
    "train_unique_idx_auto = deduplicate(train_hashes)\n",
    "valid_unique_idx_auto = deduplicate(valid_hashes)\n",
    "test_unique_idx_auto = deduplicate(test_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 200000 but corresponding boolean dimension is 10000\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def cross_deduplicate(data1, data2):\n",
    "    \"\"\"find duplicates in 2 datasets. return unique indices for dataset1.\"\"\"\n",
    "    return np.arange(data1.shape[1])[~(data2.T==data1).any(axis=0)]\n",
    "\n",
    "valid_train_unique_idx = cross_deduplicate(train_hashes, valid_hashes)\n",
    "test_train_unique_idx = cross_deduplicate(train_hashes, test_hashes)\n",
    "valid_test_unique_idx = cross_deduplicate(valid_hashes, test_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intersect unique indices for training set: \n",
    "# unique within training set, unique between train & test, unique between valid & train\n",
    "train_unique_idx = reduce(np.intersect1d, (train_unique_idx_auto, valid_train_unique_idx, test_train_unique_idx))\n",
    "valid_unique_idx = reduce(np.intersect1d, (valid_unique_idx_auto, valid_test_unique_idx))\n",
    "test_unique_idx = test_unique_idx_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 15606 duplicates from 200000 training examples.\n",
      "Removed 286 duplicates from 10000 validation examples.\n",
      "Removed 491 duplicates from 18724 validation examples.\n"
     ]
    }
   ],
   "source": [
    "# Report deduplication results\n",
    "print(\"Removed %d duplicates from %d training examples.\" % \n",
    "      (len(train_dataset) - len(train_unique_idx), len(train_dataset)))\n",
    "print(\"Removed %d duplicates from %d validation examples.\" % \n",
    "      (len(valid_dataset) - len(valid_unique_idx), len(valid_dataset)))\n",
    "print(\"Removed %d duplicates from %d validation examples.\" % \n",
    "      (len(test_dataset) - len(test_unique_idx), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_back(dataset):\n",
    "    return dataset.reshape((-1, image_size, image_size)).astype(np.float32)\n",
    "\n",
    "train_dataset_out = reformat_back(train_dataset[train_unique_idx])\n",
    "valid_dataset_out = reformat_back(valid_dataset[valid_unique_idx])\n",
    "test_dataset_out = reformat_back(test_dataset[test_unique_idx])\n",
    "train_labels_out = train_labels[train_unique_idx]\n",
    "valid_labels_out = valid_labels[valid_unique_idx]\n",
    "test_labels_out = test_labels[test_unique_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST_dedup.pickle'\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset_out,\n",
    "    'train_labels': train_labels_out,\n",
    "    'valid_dataset': valid_dataset_out,\n",
    "    'valid_labels': valid_labels_out,\n",
    "    'test_dataset': test_dataset_out,\n",
    "    'test_labels': test_labels_out,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting dataset shapes\n",
      "Training set (184394, 28, 28) (184394,)\n",
      "Validation set (9714, 28, 28) (9714,)\n",
      "Test set (18233, 28, 28) (18233,)\n"
     ]
    }
   ],
   "source": [
    "print('Resulting dataset shapes')\n",
    "print('Training set', train_dataset_out.shape, train_labels_out.shape)\n",
    "print('Validation set', valid_dataset_out.shape, valid_labels_out.shape)\n",
    "print('Test set', test_dataset_out.shape, test_labels_out.shape)"
   ]
  }
 ],
 "metadata": {
  "colabVersion": "0.3.2",
  "colab_default_view": {},
  "colab_views": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
